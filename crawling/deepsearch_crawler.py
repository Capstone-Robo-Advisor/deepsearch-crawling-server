import requests
import logging
import os

from pymongo import MongoClient
from dotenv import load_dotenv
from datetime import datetime

# Setting logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

# Load API key and MongoDB URI
load_dotenv()
API_KEY = os.getenv("DEEPSEARCH_CRAWLER_API_KEY")
MONGO_URI = os.getenv("MONGODB_URI")

# Connect to MongoDB
client = MongoClient(MONGO_URI)
db = client["deepsearch_news"]
collection = db["articles"]

def fetch_articles(page = 1, page_size = 50):
    url = "https://api-v2.deepsearch.com/v1/global-articles"
    params = {
        "date_from": "2025-04-01",
        "date_to": "2025-04-10",
        "page": page,
        "page_size": page_size,
        "api_key": API_KEY
    }

    try:
        response = requests.get(url, params=params)
        response.raise_for_status()  # 4xx/5xx ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸

        data = response.json()
        articles = data.get("data", [])

        logger.info(f"ğŸ“¡ API í˜¸ì¶œ ì„±ê³µ - í˜ì´ì§€ {page}, ìˆ˜ì‹  ê¸°ì‚¬ ìˆ˜: {len(articles)}")
        return articles

    except requests.exceptions.RequestException as e:
        logger.error(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨ - í˜ì´ì§€ {page}, ì˜¤ë¥˜: {str(e)}")
        return []

    except ValueError as e:
        logger.error(f"âŒ JSON ë””ì½”ë”© ì‹¤íŒ¨ - í˜ì´ì§€ {page}, ì˜¤ë¥˜: {str(e)}")
        return []

def save_to_mongo(articles):
    saved_count = 0
    skipped_count = 0

    for article in articles:
        # ì¤‘ë³µ ë°©ì§€ : ê¸°ì‚¬ URL ì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
        article_id = article.get("id")

        if collection.find_one({"id" : article["id"]}):
            logger.info(f"ğŸŸ¡ ì¤‘ë³µ ìŠ¤í‚µ - ê¸°ì‚¬ ID: {article_id}, ì œëª©: {article.get('title_ko', '')[:30]}...")
            skipped_count += 1
            continue

        collection.insert_one(article)
        logger.info(f"âœ… ì €ì¥ ì™„ë£Œ - ê¸°ì‚¬ ID: {article_id}, ì œëª©: {article.get('title_ko', '')[:30]}...")
        saved_count += 1

    logger.info(f"ì´ ì €ì¥: {saved_count}ê±´ / ì¤‘ë³µ ìŠ¤í‚µ: {skipped_count}ê±´")

def run_crawler():
    max_pages = 5
    for page in range(1, max_pages + 1):   # 5í˜ì´ì§€ ê¹Œì§€ ì˜ˆì‹œ
        logger.info(f"ğŸƒğŸ»í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")
        articles = fetch_articles(page=page)
        logger.info(f"â¡ï¸ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ë¨")

        save_to_mongo(articles)

        if len(articles) < 50:
            logger.info("âœ… ë§ˆì§€ë§‰ í˜ì´ì§€ì— ë„ë‹¬í–ˆìœ¼ë¯€ë¡œ í¬ë¡¤ë§ ì¢…ë£Œ")
            break


if __name__ == "__main__":
    run_crawler()
    logger.info("ì „ì²´ í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ")